{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f52581b-1594-415c-b9c9-2099b052b49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/spacy/util.py:910: UserWarning: [W095] Model 'en_core_web_sm' (3.7.1) was trained with spaCy v3.7.2 and may not be 100% compatible with the current version (3.8.0). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "/root/miniconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfCheck-NLI initialized to device cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8722a19582b4d45ac4ea0ad22356127",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filted.\n"
     ]
    }
   ],
   "source": [
    "#filt hypothesis documents\n",
    "import json\n",
    "import torch \n",
    "from selfcheckgpt.modeling_selfcheck import SelfCheckNLI\n",
    "import spacy\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from numpy import *\n",
    "from scipy.stats import entropy\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"autodl-tmp/en_core_web_sm\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "selfcheck_nli = SelfCheckNLI(device=device, nli_model='autodl-tmp/deberta-v3-large-mnli') # set device to 'cuda' if GPU is available\n",
    "\n",
    "llm_model=\"autodl-tmp/LLM-Research/Meta-Llama-3-8B-Instruct\"\n",
    "tokenizer= AutoTokenizer.from_pretrained(llm_model)\n",
    "model = AutoModelForCausalLM.from_pretrained(llm_model)\n",
    "model = model.eval()\n",
    "model = model.to(device)\n",
    " \n",
    "with open('dl19/hypothesis_documents_dl19_8', 'r') as file:\n",
    "    query_hypothesisDocuments_dl19 = json.load(file)\n",
    "    \n",
    "hypothesis_documents_only=[doc[2:] for doc in query_hypothesisDocuments_dl19]\n",
    "\n",
    "NLI_score=0.85\n",
    "fact_score=0.85\n",
    "\n",
    "for g in range(len(hypothesis_documents_only)):\n",
    "    filted_passages=[]\n",
    "    hypothesis_documents=hypothesis_documents_only[g]\n",
    "    for l in range(len(hypothesis_documents)):\n",
    "        sents_probs={}\n",
    "        passage=hypothesis_documents[l]\n",
    "        passage =  '\"' * 3 + passage + '\"' * 3 \n",
    "        passage=passage.replace(\"\\n\", \" \").replace(\"\\t\", \" \").strip()\n",
    "        sentences = [sent for sent in nlp(passage).sents] # List[spacy.tokens.span.Span]\n",
    "        sentences = [sent.text.strip() for sent in sentences if len(sent) > 3]\n",
    "\n",
    "        hypothesis_documents_left=[x for i, x in enumerate(hypothesis_documents) if i != l]\n",
    "                \n",
    "        sent_scores_nli = selfcheck_nli.predict(\n",
    "            sentences = sentences,                          # list of sentences\n",
    "            sampled_passages = hypothesis_documents_left, # list of sampled passages\n",
    "        )\n",
    "        filted_sentences_NLI=[sentence for sentence,sent_score in zip(sentences, sent_scores_nli) if sent_score<NLI_score]\n",
    "        filted_sentences_fact=[]\n",
    "        for i in range(len(sentences)):\n",
    "            input = tokenizer(sentences[i], return_tensors=\"pt\").to(device)\n",
    "            output1=model(input.input_ids,output_attentions=True)\n",
    "            logits = output1.logits\n",
    "            prob = torch.softmax(logits, dim=-1)[0]\n",
    "            probcpu=prob.cpu().detach().numpy()\n",
    "            entropies=entropy(prob.cpu().detach().numpy(), base=2,axis=-1)\n",
    "            attentions=output1.attentions\n",
    "            attentions = attentions[-1][0]\n",
    "            mean_atten = torch.sum(attentions, dim=1)\n",
    "            mean_atten = torch.mean(mean_atten, dim=0)\n",
    "            for k in range(mean_atten.shape[0]):\n",
    "                mean_atten[k] /= (mean_atten.shape[0] - k)\n",
    "            mean_atten=mean_atten.cpu().detach().numpy()\n",
    "            sent_entropyAtten=entropies[1:]@mean_atten[1:]/len(mean_atten[1:])\n",
    "            sent_probs=[]\n",
    "            for k in range(input.input_ids.size()[1]-1):\n",
    "                sent_probs.append(probcpu[k+1,input.input_ids[0][k+1]].astype(float))\n",
    "            sents_probs[sentences[i]]=sent_probs\n",
    "            if sent_entropyAtten<fact_score:\n",
    "                filted_sentences_fact.append(sentences[i])\n",
    "        filted_sentences=list(set(filted_sentences_NLI) & set(filted_sentences_fact))\n",
    "        num_tokens=0\n",
    "        tokens_probs=0\n",
    "        for p in range(len(filted_sentences)):\n",
    "            num_tokens+=len(sents_probs[filted_sentences[p]])\n",
    "            for q in range(len(sents_probs[filted_sentences[p]])):\n",
    "                tokens_probs+=sents_probs[filted_sentences[p]][q]\n",
    "        if num_tokens!=0:\n",
    "            passage_prob=tokens_probs/num_tokens\n",
    "        else:\n",
    "            passage_prob=0\n",
    "        filted_passage = ''.join(filted_sentences)\n",
    "        filted_passages.append([filted_passage,passage_prob])\n",
    "        query_hypothesisDocuments_dl19[g][2:]=filted_passages\n",
    "\n",
    "print('Filted.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a25c727-eefe-496f-bd6a-3d158c7d4552",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Oct 09, 2024 9:48:50 AM org.apache.lucene.store.MemorySegmentIndexInputProvider <init>\n",
      "INFO: Using MemorySegmentIndexInput with Java 21; to disable start with -Dorg.apache.lucene.store.MMapDirectory.enableMemorySegments=false\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Downloading https://search.maven.org/remotecontent?filepath=uk/ac/gla/dcs/terrierteam/jtreceval/0.0.5/jtreceval-0.0.5-jar-with-dependencies.jar to /root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar...\n",
      "/root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar already exists!\n",
      "Skipping download.\n",
      "Running command: ['java', '-jar', '/root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar', '-c', '-l', '2', '-m', 'map', '/root/.cache/pyserini/topics-and-qrels/qrels.dl19-passage.txt', 'dl19-lucene-top1000-trec']\n",
      "Results:\n",
      "map                   \tall\t0.3013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Downloading https://search.maven.org/remotecontent?filepath=uk/ac/gla/dcs/terrierteam/jtreceval/0.0.5/jtreceval-0.0.5-jar-with-dependencies.jar to /root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar...\n",
      "/root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar already exists!\n",
      "Skipping download.\n",
      "Running command: ['java', '-jar', '/root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar', '-c', '-m', 'ndcg_cut.10', '/root/.cache/pyserini/topics-and-qrels/qrels.dl19-passage.txt', 'dl19-lucene-top1000-trec']\n",
      "Results:\n",
      "ndcg_cut_10           \tall\t0.5058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Downloading https://search.maven.org/remotecontent?filepath=uk/ac/gla/dcs/terrierteam/jtreceval/0.0.5/jtreceval-0.0.5-jar-with-dependencies.jar to /root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar...\n",
      "/root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar already exists!\n",
      "Skipping download.\n",
      "Running command: ['java', '-jar', '/root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar', '-c', '-l', '2', '-m', 'recall.1000', '/root/.cache/pyserini/topics-and-qrels/qrels.dl19-passage.txt', 'dl19-lucene-top1000-trec']\n",
      "Results:\n",
      "recall_1000           \tall\t0.7501\n"
     ]
    }
   ],
   "source": [
    "#run BM25\n",
    "\n",
    "from pyserini.search import FaissSearcher, LuceneSearcher\n",
    "from pyserini.search.faiss import AutoQueryEncoder,AnceQueryEncoder\n",
    "import numpy as np\n",
    "from pyserini.search import get_topics, get_qrels\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "searcher = LuceneSearcher.from_prebuilt_index('msmarco-v1-passage')\n",
    "\n",
    "with open('dl19-lucene-top1000-trec', 'w')  as f:\n",
    "    for i in range(len(query_hypothesisDocuments_dl19)):\n",
    "        qid=query_hypothesisDocuments_dl19[i][0]\n",
    "        question=query_hypothesisDocuments_dl19[i][1]\n",
    "        hits = searcher.search(question, k=1000)\n",
    "        rank = 0\n",
    "        for hit in hits:\n",
    "            rank += 1\n",
    "            f.write(f'{qid} Q0 {hit.docid} {rank} {hit.score} rank\\n')\n",
    "\n",
    "!python -m pyserini.eval.trec_eval -c -l 2 -m map dl19-passage dl19-lucene-top1000-trec\n",
    "!python -m pyserini.eval.trec_eval -c -m ndcg_cut.10 dl19-passage dl19-lucene-top1000-trec\n",
    "!python -m pyserini.eval.trec_eval -c -l 2 -m recall.1000 dl19-passage dl19-lucene-top1000-trec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a278bb98-caab-4874-91c1-8e573a01c0e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Downloading https://search.maven.org/remotecontent?filepath=uk/ac/gla/dcs/terrierteam/jtreceval/0.0.5/jtreceval-0.0.5-jar-with-dependencies.jar to /root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar...\n",
      "/root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar already exists!\n",
      "Skipping download.\n",
      "Running command: ['java', '-jar', '/root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar', '-c', '-l', '2', '-m', 'map', '/root/.cache/pyserini/topics-and-qrels/qrels.dl19-passage.txt', 'dl19-lucene-top1000-trec_GOLFer']\n",
      "Results:\n",
      "map                   \tall\t0.4037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Downloading https://search.maven.org/remotecontent?filepath=uk/ac/gla/dcs/terrierteam/jtreceval/0.0.5/jtreceval-0.0.5-jar-with-dependencies.jar to /root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar...\n",
      "/root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar already exists!\n",
      "Skipping download.\n",
      "Running command: ['java', '-jar', '/root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar', '-c', '-m', 'ndcg_cut.10', '/root/.cache/pyserini/topics-and-qrels/qrels.dl19-passage.txt', 'dl19-lucene-top1000-trec_GOLFer']\n",
      "Results:\n",
      "ndcg_cut_10           \tall\t0.6063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Downloading https://search.maven.org/remotecontent?filepath=uk/ac/gla/dcs/terrierteam/jtreceval/0.0.5/jtreceval-0.0.5-jar-with-dependencies.jar to /root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar...\n",
      "/root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar already exists!\n",
      "Skipping download.\n",
      "Running command: ['java', '-jar', '/root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar', '-c', '-l', '2', '-m', 'recall.1000', '/root/.cache/pyserini/topics-and-qrels/qrels.dl19-passage.txt', 'dl19-lucene-top1000-trec_GOLFer']\n",
      "Results:\n",
      "recall_1000           \tall\t0.8241\n"
     ]
    }
   ],
   "source": [
    "#run BM25+GOLFer\n",
    "\n",
    "ratio=0.75\n",
    "\n",
    "searcher = LuceneSearcher.from_prebuilt_index('msmarco-v1-passage')\n",
    "\n",
    "with open('dl19-lucene-top1000-trec_GOLFer', 'w')  as f:\n",
    "    for i in range(len(query_hypothesisDocuments_dl19)):\n",
    "        qid=query_hypothesisDocuments_dl19[i][0]\n",
    "        query=query_hypothesisDocuments_dl19[i][1]+'.'\n",
    "        coe=int(ratio*5*8)\n",
    "        query=query*coe\n",
    "        hypothesis_documents=[x[0] for x in query_hypothesisDocuments_dl19[i][2:]]\n",
    "        hypothesis_documents=''.join(hypothesis_documents)\n",
    "        hits = searcher.search(query+hypothesis_documents, k=1000)\n",
    "        rank = 0\n",
    "        for hit in hits:\n",
    "            rank += 1\n",
    "            f.write(f'{qid} Q0 {hit.docid} {rank} {hit.score} rank\\n')\n",
    "\n",
    "!python -m pyserini.eval.trec_eval -c -l 2 -m map dl19-passage dl19-lucene-top1000-trec_GOLFer\n",
    "!python -m pyserini.eval.trec_eval -c -m ndcg_cut.10 dl19-passage dl19-lucene-top1000-trec_GOLFer\n",
    "!python -m pyserini.eval.trec_eval -c -l 2 -m recall.1000 dl19-passage dl19-lucene-top1000-trec_GOLFer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a52bf8d-b85c-427c-ae8e-f4da6a249a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "100%|██████████| 43/43 [01:33<00:00,  2.18s/it]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Downloading https://search.maven.org/remotecontent?filepath=uk/ac/gla/dcs/terrierteam/jtreceval/0.0.5/jtreceval-0.0.5-jar-with-dependencies.jar to /root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar...\n",
      "/root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar already exists!\n",
      "Skipping download.\n",
      "Running command: ['java', '-jar', '/root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar', '-c', '-l', '2', '-m', 'map', '/root/.cache/pyserini/topics-and-qrels/qrels.dl19-passage.txt', 'dl19-ance-top1000-trec']\n",
      "Results:\n",
      "map                   \tall\t0.3710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Downloading https://search.maven.org/remotecontent?filepath=uk/ac/gla/dcs/terrierteam/jtreceval/0.0.5/jtreceval-0.0.5-jar-with-dependencies.jar to /root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar...\n",
      "/root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar already exists!\n",
      "Skipping download.\n",
      "Running command: ['java', '-jar', '/root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar', '-c', '-m', 'ndcg_cut.10', '/root/.cache/pyserini/topics-and-qrels/qrels.dl19-passage.txt', 'dl19-ance-top1000-trec']\n",
      "Results:\n",
      "ndcg_cut_10           \tall\t0.6452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Downloading https://search.maven.org/remotecontent?filepath=uk/ac/gla/dcs/terrierteam/jtreceval/0.0.5/jtreceval-0.0.5-jar-with-dependencies.jar to /root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar...\n",
      "/root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar already exists!\n",
      "Skipping download.\n",
      "Running command: ['java', '-jar', '/root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar', '-c', '-l', '2', '-m', 'recall.1000', '/root/.cache/pyserini/topics-and-qrels/qrels.dl19-passage.txt', 'dl19-ance-top1000-trec']\n",
      "Results:\n",
      "recall_1000           \tall\t0.7554\n"
     ]
    }
   ],
   "source": [
    "#run ANCE\n",
    "encoder = AnceQueryEncoder(encoder_dir='autodl-tmp/ance-msmarco-passage', pooling='mean')\n",
    "searcher = FaissSearcher('autodl-tmp/msmarco-v1-passage.ance/', encoder)\n",
    "\n",
    "topics = get_topics('dl19-passage')\n",
    "qrels = get_qrels('dl19-passage')\n",
    "\n",
    "with open('dl19-ance-top1000-trec', 'w')  as f:\n",
    "    for qid in tqdm(topics):\n",
    "        if qid in qrels:\n",
    "            query = topics[qid]['title']\n",
    "            hits = searcher.search(query, k=1000)\n",
    "            rank = 0\n",
    "            for hit in hits:\n",
    "                rank += 1\n",
    "                f.write(f'{qid} Q0 {hit.docid} {rank} {hit.score} rank\\n')\n",
    "\n",
    "!python -m pyserini.eval.trec_eval -c -l 2 -m map dl19-passage dl19-ance-top1000-trec\n",
    "!python -m pyserini.eval.trec_eval -c -m ndcg_cut.10 dl19-passage dl19-ance-top1000-trec\n",
    "!python -m pyserini.eval.trec_eval -c -l 2 -m recall.1000 dl19-passage dl19-ance-top1000-trec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d82d890f-4894-4067-9b2f-82461a5aeeec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Downloading https://search.maven.org/remotecontent?filepath=uk/ac/gla/dcs/terrierteam/jtreceval/0.0.5/jtreceval-0.0.5-jar-with-dependencies.jar to /root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar...\n",
      "/root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar already exists!\n",
      "Skipping download.\n",
      "Running command: ['java', '-jar', '/root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar', '-c', '-l', '2', '-m', 'map', '/root/.cache/pyserini/topics-and-qrels/qrels.dl19-passage.txt', 'dl19-ance-top1000-trec_GOLFer']\n",
      "Results:\n",
      "map                   \tall\t0.4730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Downloading https://search.maven.org/remotecontent?filepath=uk/ac/gla/dcs/terrierteam/jtreceval/0.0.5/jtreceval-0.0.5-jar-with-dependencies.jar to /root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar...\n",
      "/root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar already exists!\n",
      "Skipping download.\n",
      "Running command: ['java', '-jar', '/root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar', '-c', '-m', 'ndcg_cut.10', '/root/.cache/pyserini/topics-and-qrels/qrels.dl19-passage.txt', 'dl19-ance-top1000-trec_GOLFer']\n",
      "Results:\n",
      "ndcg_cut_10           \tall\t0.7120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Downloading https://search.maven.org/remotecontent?filepath=uk/ac/gla/dcs/terrierteam/jtreceval/0.0.5/jtreceval-0.0.5-jar-with-dependencies.jar to /root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar...\n",
      "/root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar already exists!\n",
      "Skipping download.\n",
      "Running command: ['java', '-jar', '/root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar', '-c', '-l', '2', '-m', 'recall.1000', '/root/.cache/pyserini/topics-and-qrels/qrels.dl19-passage.txt', 'dl19-ance-top1000-trec_GOLFer']\n",
      "Results:\n",
      "recall_1000           \tall\t0.8049\n"
     ]
    }
   ],
   "source": [
    "#run ANCE+GOLFer\n",
    "\n",
    "ratio1=0.075\n",
    "ratio2=0.25\n",
    "max_tokens=128\n",
    "coe=ratio1/128*max_tokens+ratio2\n",
    "\n",
    "\n",
    "def encode_weight(query, hypothesis_documents_withweight,coe):\n",
    "    coe_passage=(1-coe)/np.sum([[row[1]] for row in hypothesis_documents_withweight])\n",
    "    prob=[[coe]]+[[row[1]*coe_passage] for row in hypothesis_documents_withweight]\n",
    "\n",
    "    hypothesis_documents=[row[0] for row in hypothesis_documents_withweight]\n",
    "\n",
    "    all_emb_c = []\n",
    "    for hypothesis_document in [query]+hypothesis_documents:\n",
    "        c=hypothesis_document\n",
    "        c_emb = encoder.encode(c)\n",
    "        all_emb_c.append(np.array(c_emb))\n",
    "    all_emb_c = np.array(all_emb_c)\n",
    "    weighted_emb_c = np.sum(prob*all_emb_c, axis=0)\n",
    "    GOLFer_vector = weighted_emb_c.reshape((1, len(weighted_emb_c)))\n",
    "    return GOLFer_vector\n",
    "    \n",
    "with open('dl19-ance-top1000-trec_GOLFer', 'w')  as f:\n",
    "    for i in range(len(query_hypothesisDocuments_dl19)):\n",
    "        qid=query_hypothesisDocuments_dl19[i][0]\n",
    "        encodedByWeight=encode_weight(query_hypothesisDocuments_dl19[i][1],query_hypothesisDocuments_dl19[i][2:],coe)\n",
    "        hits = searcher.search(encodedByWeight, k=1000)\n",
    "        rank = 0\n",
    "        for hit in hits:\n",
    "            rank += 1\n",
    "            f.write(f'{qid} Q0 {hit.docid} {rank} {hit.score} rank\\n')\n",
    "\n",
    "!python -m pyserini.eval.trec_eval -c -l 2 -m map dl19-passage dl19-ance-top1000-trec_GOLFer\n",
    "!python -m pyserini.eval.trec_eval -c -m ndcg_cut.10 dl19-passage dl19-ance-top1000-trec_GOLFer\n",
    "!python -m pyserini.eval.trec_eval -c -l 2 -m recall.1000 dl19-passage dl19-ance-top1000-trec_GOLFer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
